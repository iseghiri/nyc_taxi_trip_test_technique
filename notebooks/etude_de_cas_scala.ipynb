{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "import pandas as pd \n",
    "datas_py = pd.read_csv(\"../src/data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id2875421</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-14 17:24:55</td>\n",
       "      <td>2016-03-14 17:32:30</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.982155</td>\n",
       "      <td>40.767937</td>\n",
       "      <td>-73.964630</td>\n",
       "      <td>40.765602</td>\n",
       "      <td>N</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id2377394</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-12 00:43:35</td>\n",
       "      <td>2016-06-12 00:54:38</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.980415</td>\n",
       "      <td>40.738564</td>\n",
       "      <td>-73.999481</td>\n",
       "      <td>40.731152</td>\n",
       "      <td>N</td>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id3858529</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-19 11:35:24</td>\n",
       "      <td>2016-01-19 12:10:48</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.979027</td>\n",
       "      <td>40.763939</td>\n",
       "      <td>-74.005333</td>\n",
       "      <td>40.710087</td>\n",
       "      <td>N</td>\n",
       "      <td>2124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id3504673</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-06 19:32:31</td>\n",
       "      <td>2016-04-06 19:39:40</td>\n",
       "      <td>1</td>\n",
       "      <td>-74.010040</td>\n",
       "      <td>40.719971</td>\n",
       "      <td>-74.012268</td>\n",
       "      <td>40.706718</td>\n",
       "      <td>N</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id2181028</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-26 13:30:55</td>\n",
       "      <td>2016-03-26 13:38:10</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.973053</td>\n",
       "      <td>40.793209</td>\n",
       "      <td>-73.972923</td>\n",
       "      <td>40.782520</td>\n",
       "      <td>N</td>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  vendor_id      pickup_datetime  ... dropoff_latitude  store_and_fwd_flag  trip_duration\n",
       "0  id2875421          2  2016-03-14 17:24:55  ...        40.765602                   N            455\n",
       "1  id2377394          1  2016-06-12 00:43:35  ...        40.731152                   N            663\n",
       "2  id3858529          2  2016-01-19 11:35:24  ...        40.710087                   N           2124\n",
       "3  id3504673          2  2016-04-06 19:32:31  ...        40.706718                   N            429\n",
       "4  id2181028          2  2016-03-26 13:30:55  ...        40.782520                   N            435\n",
       "\n",
       "[5 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%python\n",
    "datas_py.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [id: string, vendor_id: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark\n",
    "      .read\n",
    "      .format(\"csv\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .load(\"../src/data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vitesse de chaque trajet \n",
    "\n",
    "J'utilise ici la formule de Haversine pour cacluler la distance en km entre un point A et un point B.\n",
    "$$a = sin^2(\\frac{lat_B-lat_A}{2})+ cos(lat_A)*cos(lat_B)*sin^2(\\frac{long_B-long_A}{2})$$\n",
    "$$d=2*r*arcsin(\\sqrt{a})$$\n",
    "\n",
    "avec r le rayon de la sphère sur laquelle sont placés les points. On considérera ici la terre comme sphérique, ce qui donne de bon résultat en pratique.\n",
    "D'après la formule de wikipédia : https://en.wikipedia.org/wiki/Haversine_formula\n",
    "\n",
    "\n",
    "On sait ensuite que :\n",
    "$$vitesse = \\frac{distance}{temps}$$\n",
    "\n",
    "On obtiendra donc ici la vitesse moyenne du trajet en km/seconde, ce qui est une unité peu parlante, mais qui permettra de comparer les trajets entre eux. On pourra par exemple voir quels sont les taxis ayant une vitesse moyenne plus rapide que les autres. Si nécessaire on pourra ensuite adapter l'unité.\n",
    "On sait bien qu'un taxi ne se déplace pas à allure constante dans les rues, cette notion de vitesse permet selon moi uniquement de comparer les trajets entre eux. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_with_speed: org.apache.spark.sql.DataFrame = [id: string, vendor_id: string ... 12 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_with_speed = df\n",
    "    .withColumn(\"a\", pow(sin(toRadians($\"dropoff_latitude\" - $\"pickup_latitude\") / 2), 2) + cos(toRadians($\"pickup_latitude\")) * cos(toRadians($\"dropoff_latitude\")) * pow(sin(toRadians($\"dropoff_longitude\" - $\"pickup_longitude\") / 2), 2))\n",
    "    .withColumn(\"distance\", asin(sqrt(col(\"a\"))) * 2 * 6371)\n",
    "    .withColumn(\"speed\",$\"distance\" / $\"trip_duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Array[org.apache.spark.sql.Row] = Array([id2875421,2,2016-03-14 17:24:55,2016-03-14 17:32:30,1,-73.982154846191406,40.767936706542969,-73.964630126953125,40.765602111816406,N,455,1.3830896636179652E-8,1.498520779647477,0.003293452262961488], [id2377394,1,2016-06-12 00:43:35,2016-06-12 00:54:38,1,-73.980415344238281,40.738563537597656,-73.999481201171875,40.731151580810547,N,663,2.0078128522717423E-8,1.805507168795824,0.002723238565302902], [id3858529,2,2016-01-19 11:35:24,2016-01-19 12:10:48,1,-73.979026794433594,40.763938903808594,-74.005332946777344,40.710086822509766,N,2124,2.511076618143245E-7,6.385098495252941,0.0030061668998366013], [id3504673,2,2016-04-06 19:32:31,2016-04-06 19:39:40,1,-74.010040283203125,40.719970703125,-74.01226806640625,40.706718444824219,N,429,1.3591556..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_speed.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nombre de trajets effectués en fonction du jour de la semaine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_with_week_day: org.apache.spark.sql.DataFrame = [id: string, vendor_id: string ... 11 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_with_week_day = df.withColumn(\"pickup_datetime\",\n",
    "    to_timestamp(col(\"pickup_datetime\")))\n",
    "    .withColumn(\"week_day_number\", date_format(col(\"pickup_datetime\"), \"u\"))\n",
    "    .withColumn(\"week_day\", date_format(col(\"pickup_datetime\"), \"E\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_with_trip_per_day: org.apache.spark.rdd.RDD[(Any, Int)] = ShuffledRDD[20] at reduceByKey at <console>:28\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_with_trip_per_day = df_with_week_day.rdd\n",
    "        .map(x => (x(12),1))\n",
    "        .reduceByKey(_+_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[(Any, Int)] = Array((Mon,187418), (Tue,202749), (Sun,195366), (Fri,223533), (Thu,218574), (Sat,220868), (Wed,210136))\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_trip_per_day.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise ensuite la méthode du MapReduce pour calculer le nombre de trajet en fonction du jour de la semaine.\n",
    "Pourquoi utiliser le MapReduce avec le `reduceByKey` et non pas le `groupByKey`.\n",
    "Lorsque le montant de données est élévé, `groupByKey` est moins performant que `reduceByKey`. Voyons pourquoi.\n",
    "\n",
    "\n",
    "Lorsqu'on utilise les RDD, chaque morceau de RDD est appelé partition. Chaque partition appartient à un executeur.\n",
    "`groupByKey` va chercher à regrouper les clés identiques dans la même partitition pour ensuite faire le traitement.\n",
    "Il y a deux conséquences à cela :\n",
    "\n",
    "- Cela produit un shuffle assez important des données car il doit le faire pour chaque donnée de chaque partition du RDD. Donc, si les partitions ne sont pas sur une même machine, cela provoque un gros trafic réseau.\n",
    "- Si dans notre exemple les trajets avaient lieu à 99% le samedi, groupByKey rassemblera toutes les données ayant comme clé \"samedi\" dans une seule machine, ce qui peut causer des problèmes de mémoire.\n",
    "\n",
    "Pour `reduceByKey`cela se passe autrement. Il y a d'abord un prétraitement dans chaque partition. Ensuite les nouvelles clé/valeur sont déplacées dans les partitions selon leur clé pour avoir leur traitement final.\n",
    "La quantité de données est donc moins élevée, réduisant ainsi le temps de traitement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nombre de trajets effectués en fonction de la tranche horaire\n",
    "\n",
    "\n",
    "on divisera les tranche horaire en fonction de l'heure de départ du taxi : \n",
    "- 00:00 - 04:00 -> tranche horaire 1\n",
    "- 04:00 - 08:00 -> tranche horaire 2 \n",
    "- 08:00 - 12:00 -> tranche horaire 3\n",
    "- 12:00 - 16:00 -> tranche horaire 4\n",
    "- 16:00 - 20:00 -> tranche horaire 5\n",
    "- 20:00 - 23:59 -> tranche horaire 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_with_time_slice: org.apache.spark.sql.DataFrame = [id: string, vendor_id: string ... 11 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_with_time_slice = df\n",
    "    .withColumn(\"pickup_datetime\",to_timestamp(col(\"pickup_datetime\"))) \n",
    "    .withColumn(\"hour_pickup\", date_format(col(\"pickup_datetime\"), \"hh:mm\"))\n",
    "    .withColumn(\"time_slice\",\n",
    "        when((col(\"hour_pickup\")).geq(\"00:00\") && (col(\"hour_pickup\").lt(\"04:00\")), \"slice_1\")\n",
    "        .when((col(\"hour_pickup\")).geq(\"04:00\") && (col(\"hour_pickup\").lt(\"08:00\")), \"slice_2\")\n",
    "        .when((col(\"hour_pickup\")).geq(\"08:00\") && (col(\"hour_pickup\").lt(\"12:00\")), \"slice_3\")\n",
    "        .when((col(\"hour_pickup\")).geq(\"12:00\") && (col(\"hour_pickup\").lt(\"16:00\")), \"slice_4\")\n",
    "        .when((col(\"hour_pickup\")).geq(\"16:00\") && (col(\"hour_pickup\").lt(\"20:00\")), \"slice_5\")\n",
    "        .when((col(\"hour_pickup\")).geq(\"20:00\") && (col(\"hour_pickup\").leq(\"23:59\")), \"slice_6\")\n",
    "        .otherwise(\"slice_unknown\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_with_trip_per_time_slice: org.apache.spark.rdd.RDD[(Any, Int)] = ShuffledRDD[29] at reduceByKey at <console>:28\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_with_trip_per_time_slice = df_with_time_slice.rdd\n",
    "    .map(x => (x(12),1))\n",
    "    .reduceByKey(_+_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[(Any, Int)] = Array((slice_1,305014), (slice_2,441346), (slice_3,587163), (slice_4,125121))\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_trip_per_time_slice.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nombre de km par jour de la semaine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_with_distance_per_day_of_week: org.apache.spark.sql.DataFrame = [id: string, vendor_id: string ... 13 more fields]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_with_distance_per_day_of_week = df \n",
    "    .withColumn(\"a\", pow(sin(toRadians($\"dropoff_latitude\" - $\"pickup_latitude\") / 2), 2) + cos(toRadians($\"pickup_latitude\")) * cos(toRadians($\"dropoff_latitude\")) * pow(sin(toRadians($\"dropoff_longitude\" - $\"pickup_longitude\") / 2), 2))\n",
    "    .withColumn(\"distance\", asin(sqrt(col(\"a\"))) * 2 * 6371)\n",
    "    .withColumn(\"pickup_datetime\",to_timestamp(col(\"pickup_datetime\"))) \n",
    "    .withColumn(\"week_day_number\", date_format(col(\"pickup_datetime\"), \"u\")) \n",
    "    .withColumn(\"week_day\", date_format(col(\"pickup_datetime\"), \"E\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- trip_duration: string (nullable = true)\n",
      " |-- a: double (nullable = true)\n",
      " |-- distance: double (nullable = true)\n",
      " |-- week_day_number: string (nullable = true)\n",
      " |-- week_day: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_distance_per_day_of_week.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "week_day_abb_index: Int = 14\n",
       "distance_index: Int = 12\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val week_day_abb_index = df_with_distance_per_day_of_week.columns.indexOf(\"week_day\")\n",
    "val distance_index = df_with_distance_per_day_of_week.columns.indexOf(\"distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour ce dernier calcul je ne comprends malheuresement pas où se trouve mon erreur. J'ai réussi à l'implémenter en pyspark, néanmoins je ne trouve pas comment gérer se problème de type, ni comment cast la variable dans le bon type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "34: error: type mismatch;",
     "output_type": "error",
     "traceback": [
      "<console>:34: error: type mismatch;",
      " found   : Any",
      " required: String",
      "           .reduceByKey((x,y) => x + y)",
      "                                     ^",
      ""
     ]
    }
   ],
   "source": [
    "val df_with_km_par_time_slice = df_with_distance_per_day_of_week.rdd\n",
    "    .map(x => (x(week_day_abb_index),x(distance_index)))\n",
    "    .reduceByKey((x,y) => x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
